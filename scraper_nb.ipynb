{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "webscraper\n"
     ]
    }
   ],
   "source": [
    "from webscraper import Scraper\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.common.exceptions import NoSuchElementException\n",
    "# from selenium.webdriver import Chrome \n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import csv\n",
    "from getpass import getpass \n",
    "import time \n",
    "from time import sleep \n",
    "\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "#depending on the browser used \n",
    "#from msedge.selenium_tools import Edge, EdgeOptions\n",
    "#from selenium.webdriver import Chrome \n",
    "#from selenium.webdriver import Firefox \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "bot = Scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.login_username()\n",
    "bot.login_password()\n",
    "bot.login()\n",
    "bot.accept_cookies()\n",
    "bot.search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "time.sleep(1)\n",
    "container = bot.driver.find_element(By.XPATH,'//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div/div/div[2]/div/section/div/div')\n",
    "twitter_list = container.find_elements(By.XPATH, './div')\n",
    "last_postion = bot.driver.execute_script(\"return window.pageYOffset;\")\n",
    "scrolling = True\n",
    "dict_data = {'UUID': [],'twitter_username':[] , 'twitter_handle': [] ,'twitter_postdate': [],'twitter_comment':[],'twitter_reply_cnt':[],'twitter_like_cnt':[],'twitter_retweet_cnt':[]}\n",
    "\n",
    "last_postion = bot.driver.execute_script(\"return window.pageYOffset;\")\n",
    "scrolling = True\n",
    "\n",
    "while True:\n",
    "    container = bot.driver.find_element(By.XPATH,'//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div/div/div[2]/div/section/div/div')\n",
    "    twitter_list = container.find_elements(By.XPATH, './div')\n",
    "\n",
    "    for tweet in twitter_list:\n",
    "        try:\n",
    "            dict_data['twitter_username'].append(tweet.find_element_by_xpath('.//span').text)\n",
    "            dict_data['twitter_handle'].append(tweet.find_element_by_xpath('.//span[contains(text(),\"@\")]').text)\n",
    "            try:\n",
    "                dict_data['twitter_postdate'].append(tweet.find_element_by_xpath('.//time').get_attribute('datetime'))\n",
    "            except NoSuchElementException:\n",
    "                dict_data['twitter_postdate'].append('advert')\n",
    "            # dict_data['twitter_comment'].append(tweet.find_element_by_xpath('.//div[2]/div[2]/div[1]').text)\n",
    "        #responding =container.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "        #text=comment + responding\n",
    "            #dict_data['twitter_comment'].append(tweet.find_element_by_xpath('//div[@lang =\"en\"]').text)\n",
    "            dict_data['twitter_comment'].append(tweet.find_element_by_xpath('//div[@class =\"css-1dbjc4n\"]').text)\n",
    "            dict_data['twitter_reply_cnt'].append(tweet.find_element_by_xpath('//div[@data-testid =\"reply\"]').text)\n",
    "\n",
    "            dict_data['twitter_like_cnt'].append(tweet.find_element_by_xpath('//div[@data-testid =\"like\"]').text)\n",
    "\n",
    "            dict_data['twitter_retweet_cnt'].append(tweet.find_element_by_xpath('//div[@data-testid =\"retweet\"]').text)\n",
    "            dict_data['UUID'].append(uuid.uuid4()) \n",
    "        except StaleElementReferenceException or NoSuchElementException:\n",
    "            sleep(4)  \n",
    "    \n",
    "    Scroll_attempt = 0\n",
    "    while True:\n",
    "        bot.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1)\n",
    "        curr_position = bot.driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_postion == curr_position:\n",
    "            Scroll_attempt += 1\n",
    "            if Scroll_attempt >= 3:\n",
    "                scrolling=False\n",
    "                break\n",
    "            else:\n",
    "                sleep(2)\n",
    "        else:\n",
    "            last_postion =curr_position\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8807417fc3cb341cf94aa1793afadd53b3c6890d8eb1ac10f50cd84fc834cbf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
